<div class="container my-5">
  <h1 class="display-4 text-primary mb-4 text-center"><u>Titanic Survival Prediction</u></h1>
  <br>
  <p class="lead mb-4 text-center">
    In this project, I conducted an in-depth analysis of the Titanic dataset from Kaggle to develop and fine-tune decision tree models for predicting passenger survival outcomes. The following outlines the comprehensive data processing steps undertaken to enhance model accuracy and reliability.
  </p>
  <h3 class="mb-4">Data Preparation and Cleaning</h3>
  <p class="mb-4">
    The initial phase involved cleaning and preparing the dataset by addressing the following key tasks:
  </p>
  <ul>
    <li><strong>Attribute Selection:</strong> I excluded non-relevant attributes such as PassengerId, Ticket Number, and Cabin Number. These attributes were deemed non-contributory to the survival prediction.</li>
    <li><strong>Focus on Key Attributes:</strong> The analysis was centered around the crucial attributes: Passenger Class, Name, Sex, Age, Number of siblings/spouses, Number of parents/children, Fare, and Embarkation Port.</li>
  </ul>
  <h3 class="mb-4">Handling Missing Data</h3>
  <p class="mb-4">
    Missing values were handled using a range of imputation techniques to ensure data completeness and improve model performance:
  </p>
  <ul>
    <li><strong>Mode Imputation:</strong> For categorical variables, missing values were filled with the most frequently occurring value (mode). This method ensures that the imputed values reflect the common category within the dataset.</li>
    <li><strong>Median Imputation:</strong> For numeric attributes, missing values were replaced with the median value. This approach minimizes the impact of outliers, providing a robust central measure for imputation.</li>
    <li><strong>K-Nearest Neighbors (KNN) Imputation:</strong> KNN Imputation estimates missing values based on the values of the nearest neighbors in the dataset. The algorithm identifies the K most similar records (neighbors) to the record with the missing value and uses their values to predict the missing data. This method considers the similarity between records, offering a context-sensitive approach to imputation.</li>
    <li><strong>Multivariate Imputation by Chained Equations (MICE):</strong> MICE is an advanced imputation technique that models each feature with missing values as a function of other features. The process involves multiple iterations where missing values are estimated based on predictions from a regression model built on the other variables. This method accounts for the relationships between multiple variables, providing a comprehensive imputation strategy.</li>
  </ul>
  <h3 class="mb-4">Feature Engineering</h3>
  <p class="mb-4">
    To capture more nuanced patterns in the data, I performed feature engineering, which included:
  </p>
  <ul>
    <li><strong>Categorization of Continuous Variables:</strong> Age and Fare were divided into meaningful categories to better reflect their impact on survival chances (e.g., Fare < 50, 50 - 100, 100 - 200, > 200).</li>
    <li><strong>New Feature Creation:</strong> Introduced new features such as <code>isAlone</code>, indicating whether a passenger was traveling alone, and <code>title</code>, extracted from passenger names to capture social and cultural information.</li>
  </ul>
  <h3 class="mb-4">Model Development and Evaluation</h3>
  <p class="mb-4">
    With the prepared dataset, several decision tree models were developed and evaluated. Each iteration refined the approach, resulting in the following improvements:
  </p>
  <ul>
    <li><strong>Decision Tree Version 1:</strong> Achieved an initial accuracy of 61%.</li>
    <li><strong>Decision Tree Version 2:</strong> Improved accuracy to 70% by categorizing continuous variables.</li>
    <li><strong>Decision Tree Versions 3 to 6:</strong> Implemented advanced imputation techniques, maintaining accuracy around 70%.</li>
    <li><strong>Decision Tree Version 7:</strong> Introduced feature engineering, resulting in a final accuracy of 74%.</li>
  </ul>
  <p class="mb-4">
    This structured approach to data processing and feature engineering significantly enhanced the predictive power of the decision tree models, offering valuable insights into the factors influencing Titanic survival rates.
  </p>

  <br>

  <h2 class="mb-4"><u>Decision Tree Models Overview</u></h2>

  <div ngbAccordion #acc="ngbAccordion" class="accordion">
    <!-- Decision Tree 1 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v1</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>In Decision Tree Model v1, we focused on using the most relevant attributes identified from the dataset. The chosen attributes were:</p>
          <ul>
            <li>Sex</li>
            <li>Passenger Class</li>
            <li>Number of co-travelers</li>
            <li>Age</li>
            <li>Fare</li>
            <li>Embarkation port</li>
          </ul>
          <p>Model performance was initially evaluated, with an accuracy of about 61%. This version served as a foundation for further enhancements.</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 2 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v2</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>For Decision Tree Model v2, we made significant improvements by categorizing attributes more effectively:</p>
          <ul>
            <li>Properly categorizing attributes in our training set (e.g., sex and embarked as integers).</li>
            <li>Dividing fare and age into different categories (e.g., fare < 50, 50 - 100, 100 - 200, > 200).</li>
          </ul>
          <p>This version aimed to refine the model's accuracy by addressing previous shortcomings. By splitting the age and fare into distinct categories, we observed an increase to 70% accuracy (+9%).</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 3 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v3</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>Decision Tree Model v3 implemented the first imputation method:</p>
          <ul>
            <li><strong>Mode Imputation:</strong> Missing values are replaced with the most frequently occurring value (mode) in the column.</li>
          </ul>
          <p>Despite this method, the accuracy achieved was around 66%, which was slightly lower than version 2 but similar to the subsequent versions.</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 4 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v4</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>In Decision Tree Model v4, we utilized the second imputation method:</p>
          <ul>
            <li><strong>Median Imputation:</strong> Missing values are replaced with the median value of the column, which is less sensitive to outliers compared to the mode.</li>
          </ul>
          <p>Accuracy for this version was around 67%, indicating marginal improvement over version 3 but still not surpassing version 2.</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 5 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v5</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>Decision Tree Model v5 applied the third imputation method:</p>
          <ul>
            <li><strong>KNN (K-Nearest Neighbors) Imputation:</strong> Missing values are estimated based on the values of the nearest neighbors in the feature space.</li>
          </ul>
          <p>This method achieved an accuracy of 70%, matching the results from version 2.</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 6 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v6</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>For Decision Tree Model v6, we employed the fourth imputation method:</p>
          <ul>
            <li><strong>MICE (Multivariate Imputation by Chained Equations):</strong> This method performs multiple imputations based on a series of regressions on other features to estimate missing values.</li>
          </ul>
          <p>The accuracy achieved with MICE was consistent at 70%, similar to version 5 and version 2.</p>
        </div>
      </div>
    </div>

    <!-- Decision Tree 7 -->
    <div ngbAccordionItem>
      <h2 ngbAccordionHeader>
        <button ngbAccordionButton>Decision Tree Model v7</button>
      </h2>
      <div ngbAccordionCollapse>
        <div ngbAccordionBody>
          <p>In Decision Tree Model v7, we introduced feature engineering with two new features:</p>
          <ul>
            <li>isAlone: Indicates whether the passenger is traveling alone.</li>
            <li>title: A title extracted from the passenger's name.</li>
          </ul>
          <p>These new features contributed to a noticeable increase in accuracy, reaching approximately 74% accuracy.</p>
        </div>
      </div>
    </div>
  </div>

  <div class="d-flex flex-column align-items-center my-5">
    <button type="button" class="btn btn-primary btn-lg mb-3" [routerLink]="['/decision-tree']">
      Try it out yourself!
    </button>
  </div>

  <footer class="mt-5">
    <p class="text-muted">Â© 2024 Titanic Survival Prediction. All rights reserved.</p>
  </footer>
</div>
